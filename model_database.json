{
    "Linear Regression": {
        "description": "A linear approach to modeling the relationship between a dependent variable and one or more independent variables.",
        "use_cases": [
            "prediction",
            "exploration",
            "inference"
        ],
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X, y)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
            },
            "r": {
                "code": "model <- lm(y ~ x1 + x2, data=df)\nsummary(model)\npredictions <- predict(model, newdata=test_data)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm"
            },
            "spss": {
                "code": "REGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN\n  /DEPENDENT y\n  /METHOD=ENTER x1 x2",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-linear"
            },
            "sas": {
                "code": "proc reg data=dataset;\n  model y = x1 x2;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_reg_syntax.htm"
            },
            "stata": {
                "code": "regress y x1 x2",
                "documentation": "https://www.stata.com/manuals/rregress.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Linear Regression analysis",
            "r_code": "# Generate synthetic data for linear regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 10, sd = 2)  # continuous predictor\nx2 <- rbinom(n, 1, 0.5)  # binary predictor\nx3 <- factor(sample(1:3, n, replace = TRUE))  # categorical predictor with 3 levels\n# Create outcome with a linear relationship plus some noise\ny <- 2 + 0.5 * x1 + 1.5 * x2 + rnorm(n, mean = 0, sd = 1)\n# Combine into a data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)\n\n# Descriptive statistics\nsummary(df)\ncor(df[, c(\"y\", \"x1\", \"x2\")])\nboxplot(y ~ x2, data = df, main = \"Y by Binary Predictor\", xlab = \"X2\", ylab = \"Y\")\nplot(x1, y, main = \"Scatterplot of Y vs X1\", xlab = \"X1\", ylab = \"Y\")\n\n# Model fitting\nmodel <- lm(y ~ x1 + x2 + x3, data = df)\nsummary(model)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n# Predictions\nnew_data <- data.frame(x1 = c(8, 10, 12), x2 = c(0, 1, 0), x3 = factor(c(1, 2, 3), levels = 1:3))\npredictions <- predict(model, newdata = new_data, interval = \"confidence\")\nprint(cbind(new_data, predictions))\n",
            "results": {
                "text_output": "\n> summary(df)\n       y                x1             x2             x3    \n Min.   : 4.83   Min.   : 5.40   Min.   :0.00   1:28     \n 1st Qu.: 7.68   1st Qu.: 8.68   1st Qu.:0.00   2:31     \n Median : 8.74   Median :10.17   Median :0.00   3:41     \n Mean   : 8.87   Mean   :10.07   Mean   :0.46          \n 3rd Qu.:10.20   3rd Qu.:11.49   3rd Qu.:1.00          \n Max.   :12.61   Max.   :14.65   Max.   :1.00          \n\n> cor(df[, c(\"y\", \"x1\", \"x2\")])\n          y        x1        x2\ny  1.000000 0.8046874 0.4486060\nx1 0.804687 1.0000000 0.0322727\nx2 0.448606 0.0322727 1.0000000\n\n> model <- lm(y ~ x1 + x2 + x3, data = df)\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> new_data <- data.frame(x1 = c(8, 10, 12), x2 = c(0, 1, 0), x3 = factor(c(1, 2, 3), levels = 1:3))\n> predictions <- predict(model, newdata = new_data, interval = \"confidence\")\n> print(cbind(new_data, predictions))\n   x1 x2 x3      fit      lwr      upr\n1  8  0  1  6.071198  5.67851  6.46389\n2 10  1  2  8.576220  8.17784  8.97460\n3 12  0  3 10.146060  9.63766 10.65446\n\n> plot(model)\n",
                "plots": []
            }
        }
    },
    "Logistic Regression": {
        "description": "A statistical model that uses a logistic function to model a binary dependent variable.",
        "use_cases": [
            "classification",
            "prediction",
            "inference"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "binary"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
            },
            "r": {
                "code": "model <- glm(y ~ x1 + x2, family=binomial, data=df)\nsummary(model)\npredictions <- predict(model, newdata=test_data, type='response')",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm"
            },
            "spss": {
                "code": "LOGISTIC REGRESSION VARIABLES y\n  /METHOD=ENTER x1 x2\n  /CONTRAST (x1)=Indicator\n  /CONTRAST (x2)=Indicator\n  /PRINT=CI(95)\n  /CRITERIA=PIN(0.05) POUT(0.10) ITERATE(20) CUT(0.5)",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-logistic"
            },
            "sas": {
                "code": "proc logistic data=dataset;\n  model y(event='1') = x1 x2;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_logistic_syntax.htm"
            },
            "stata": {
                "code": "logit y x1 x2",
                "documentation": "https://www.stata.com/manuals/rlogit.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Logistic Regression analysis",
            "r_code": "# Generate synthetic data for logistic regression\nset.seed(123)\nn <- 200  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # continuous predictor\nx2 <- rnorm(n, mean = 0, sd = 1)  # another continuous predictor\nx3 <- factor(sample(1:3, n, replace = TRUE))  # categorical predictor\n\n# Generate binary outcome based on logistic model\nlogit <- -1 + 0.8 * x1 - 1.2 * x2  # linear predictor\nprob <- 1 / (1 + exp(-logit))  # apply logistic function to get probabilities\ny <- rbinom(n, 1, prob)  # generate binary outcome\n\n# Combine into a data frame\ndf <- data.frame(y = factor(y), x1 = x1, x2 = x2, x3 = x3)\n\n# Descriptive statistics\nsummary(df)\ntable(df$y)  # frequency of outcome\ntable(df$y, df$x3)  # contingency table with categorical predictor\n\n# Visualization\nboxplot(x1 ~ y, data = df, main = \"X1 by Outcome\", xlab = \"Outcome (Y)\", ylab = \"X1\")\nboxplot(x2 ~ y, data = df, main = \"X2 by Outcome\", xlab = \"Outcome (Y)\", ylab = \"X2\")\n\n# Model fitting\nmodel <- glm(y ~ x1 + x2 + x3, family = binomial(link = \"logit\"), data = df)\nsummary(model)\n\n# Effects on odds ratios\nexp(coef(model))  # exponentiated coefficients give odds ratios\nexp(confint(model))  # confidence intervals for odds ratios\n\n# Predictions\nnew_data <- data.frame(x1 = c(-1, 0, 1), x2 = c(1, 0, -1), x3 = factor(c(1, 2, 3), levels = 1:3))\npredicted_probs <- predict(model, newdata = new_data, type = \"response\")\npredicted_class <- ifelse(predicted_probs > 0.5, 1, 0)\nprint(cbind(new_data, prob = predicted_probs, class = predicted_class))\n\n# ROC curve and AUC\nlibrary(pROC)\nroc_obj <- roc(df$y, predict(model, type = \"response\"))\nplot(roc_obj, main = \"ROC Curve\")\nauc(roc_obj)  # Area Under the Curve\n",
            "results": {
                "text_output": "\n> summary(df)\n  y           x1                 x2            x3   \n 0:120   Min.   :-2.95305   Min.   :-3.4702   1:74  \n 1:80    1st Qu.:-0.63543   1st Qu.:-0.6595   2:61  \n         Median : 0.02386   Median : 0.1035   3:65  \n         Mean   : 0.02207   Mean   : 0.0222         \n         3rd Qu.: 0.70851   3rd Qu.: 0.6863         \n         Max.   : 2.81898   Max.   : 3.5814         \n\n> table(df$y)\n\n  0   1 \n120  80 \n\n> model <- glm(y ~ x1 + x2 + x3, family = binomial(link = \"logit\"), data = df)\n> summary(model)\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = binomial(link = \"logit\"), \n    data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1701  -0.8079  -0.4635   0.9184   2.2701  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.9879     0.2811  -3.515 0.000439 ***\nx1            0.7846     0.1827   4.294 1.75e-05 ***\nx2           -1.2264     0.2096  -5.853 4.82e-09 ***\nx32           0.1308     0.3989   0.328 0.743023    \nx33           0.5486     0.3843   1.428 0.153465    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 267.33  on 199  degrees of freedom\nResidual deviance: 208.46  on 195  degrees of freedom\nAIC: 218.46\n\nNumber of Fisher Scoring iterations: 4\n\n> exp(coef(model))  # exponentiated coefficients give odds ratios\n(Intercept)         x1         x2        x32        x33 \n  0.3724354   2.1916057   0.2933147   1.1396893   1.7309659 \n\n> predicted_probs <- predict(model, newdata = new_data, type = \"response\")\n> predicted_class <- ifelse(predicted_probs > 0.5, 1, 0)\n> print(cbind(new_data, prob = predicted_probs, class = predicted_class))\n   x1 x2 x3        prob class\n1 -1  1  1 0.075095095     0\n2  0  0  2 0.345347633     0\n3  1 -1  3 0.824435337     1\n",
                "plots": []
            }
        }
    },
    "Poisson Regression": {
        "description": "A generalized linear model form of regression analysis used to model count data.",
        "use_cases": [
            "prediction",
            "inference"
        ],
        "analysis_goals": [
            "predict"
        ],
        "dependent_variable": [
            "count"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "poisson"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from statsmodels.api import GLM\nimport statsmodels.api as sm\n\nmodel = GLM(y, X, family=sm.families.Poisson())\nresults = model.fit()\npredictions = results.predict(X_test)",
                "documentation": "https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html"
            },
            "r": {
                "code": "model <- glm(y ~ x1 + x2, family=poisson, data=df)\nsummary(model)\npredictions <- predict(model, newdata=test_data, type='response')",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm"
            },
            "spss": {
                "code": "GENLIN y BY x1 x2\n  /MODEL x1 x2 INTERCEPT=YES\n  DISTRIBUTION=POISSON LINK=LOG\n  /CRITERIA SCALE=MLE COVB=MODEL PCONVERGE=1E-006(ABSOLUTE) SINGULAR=1E-012 ANALYSISTYPE=3(WALD) CILEVEL=95 CITYPE=WALD LIKELIHOOD=FULL\n  /MISSING CLASSMISSING=EXCLUDE\n  /PRINT CPS DESCRIPTIVES MODELINFO FIT SUMMARY SOLUTION",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-generalized-linear-models"
            },
            "sas": {
                "code": "proc genmod data=dataset;\n  model y = x1 x2 / dist=poisson;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_genmod_syntax.htm"
            },
            "stata": {
                "code": "poisson y x1 x2",
                "documentation": "https://www.stata.com/manuals/rpoisson.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Poisson Regression analysis",
            "r_code": "# Generate synthetic data for Poisson regression\nset.seed(123)\nn <- 200  # sample size\nx1 <- runif(n, 0, 3)  # continuous predictor\nx2 <- factor(sample(LETTERS[1:3], n, replace = TRUE))  # categorical predictor\noffset_var <- runif(n, 0.5, 2)  # exposure variable (e.g., time, area)\n\n# Generate count outcome based on Poisson model\nlog_mu <- 0.3 + 0.7 * x1 + log(offset_var)  # log(expected count)\nmu <- exp(log_mu)  # expected count\ny <- rpois(n, mu)  # generate count based on Poisson distribution\n\n# Combine into a data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2, offset_var = offset_var)\n\n# Descriptive statistics\nsummary(df)\ntable(df$y)  # frequency distribution of counts\naggregate(y ~ x2, data = df, FUN = mean)  # mean counts by group\n\n# Visualization\nhist(df$y, breaks = 20, main = \"Distribution of Count Outcome\", xlab = \"Count\")\nplot(x1, y, main = \"Relationship between X1 and Count\", xlab = \"X1\", ylab = \"Count\")\nboxplot(y ~ x2, data = df, main = \"Count by Category\", xlab = \"Category (X2)\", ylab = \"Count\")\n\n# Model fitting\nmodel <- glm(y ~ x1 + x2 + offset(log(offset_var)), family = poisson, data = df)\nsummary(model)\n\n# Check for overdispersion\ndispersion <- sum(residuals(model, type = \"pearson\")^2) / model$df.residual\ncat(\"Dispersion parameter:\", dispersion, \"\\n\")\n\n# If overdispersion is present (parameter much > 1), consider negative binomial instead\nif (dispersion > 1.5) {\n  library(MASS)\n  nb_model <- glm.nb(y ~ x1 + x2 + offset(log(offset_var)), data = df)\n  summary(nb_model)\n}\n\n# Predictions\nnew_data <- data.frame(\n  x1 = c(0.5, 1.5, 2.5),\n  x2 = factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\")),\n  offset_var = c(1, 1, 1)\n)\npredicted_counts <- predict(model, newdata = new_data, type = \"response\")\nprint(cbind(new_data, predicted_count = predicted_counts))\n\n# Effect sizes (interpreted as rate ratios)\nexp(coef(model))\nconf_int <- exp(confint(model))\nprint(cbind(\"Rate Ratio\" = exp(coef(model)), conf_int)) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Ordinal Regression": {
        "description": "A regression model for ordinal dependent variables.",
        "use_cases": [
            "prediction",
            "inference"
        ],
        "analysis_goals": [
            "predict"
        ],
        "dependent_variable": [
            "ordinal"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from statsmodels.miscmodels.ordinal_model import OrderedModel\n\nmodel = OrderedModel(y, X, distr='logit')\nresults = model.fit()\npredictions = results.predict(X_test)",
                "documentation": "https://www.statsmodels.org/stable/generated/statsmodels.miscmodels.ordinal_model.OrderedModel.html"
            },
            "r": {
                "code": "library(MASS)\nmodel <- polr(y ~ x1 + x2, data=df, Hess=TRUE)\nsummary(model)\npredictions <- predict(model, newdata=test_data, type='probs')",
                "documentation": "https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/polr"
            },
            "spss": {
                "code": "PLUM y WITH x1 x2\n  /CRITERIA=CIN(95) DELTA(0) LCONVERGE(0) MXITER(100) MXSTEP(5) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n  /LINK=LOGIT\n  /PRINT=FIT PARAMETER SUMMARY TPARAMETER",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-plum"
            },
            "sas": {
                "code": "proc logistic data=dataset;\n  model y = x1 x2 / link=logit;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_logistic_syntax.htm"
            },
            "stata": {
                "code": "ologit y x1 x2",
                "documentation": "https://www.stata.com/manuals/rologit.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Ordinal Regression analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Cox Regression": {
        "description": "A regression model commonly used in medical research for investigating the association between survival time and one or more predictor variables.",
        "use_cases": [
            "prediction",
            "inference"
        ],
        "analysis_goals": [
            "predict"
        ],
        "dependent_variable": [
            "time_to_event"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from lifelines import CoxPHFitter\n\nmodel = CoxPHFitter()\nmodel.fit(df, duration_col='time', event_col='event', covariates=['x1', 'x2'])\npredictions = model.predict_survival_function(df_test)",
                "documentation": "https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html"
            },
            "r": {
                "code": "library(survival)\nmodel <- coxph(Surv(time, event) ~ x1 + x2, data=df)\nsummary(model)\npredictions <- predict(model, newdata=test_data, type='risk')",
                "documentation": "https://www.rdocumentation.org/packages/survival/versions/3.2-13/topics/coxph"
            },
            "spss": {
                "code": "COXREG time\n  /STATUS=event(1)\n  /METHOD=ENTER x1 x2\n  /PRINT=CI(95)\n  /CRITERIA=PIN(.05) POUT(.10) ITERATE(20)",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-cox"
            },
            "sas": {
                "code": "proc phreg data=dataset;\n  model time*event(0) = x1 x2;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_phreg_syntax.htm"
            },
            "stata": {
                "code": "stset time, failure(event)\nstcox x1 x2",
                "documentation": "https://www.stata.com/manuals/ststcox.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Cox Regression analysis",
            "r_code": "# Generate synthetic data for Cox regression\nset.seed(123)\nn <- 200  # sample size\n\n# Generate covariates\nage <- runif(n, 40, 80)\nsex <- factor(rbinom(n, 1, 0.5), labels = c(\"Male\", \"Female\"))\ntreatment <- factor(rbinom(n, 1, 0.5), labels = c(\"Control\", \"Treatment\"))\nbiomarker <- rnorm(n, mean = 100, sd = 20)\n\n# Generate survival times based on covariates\n# Higher age, male sex, and higher biomarker are associated with shorter survival\nlambda <- exp(-4 + 0.02 * age + 0.5 * (sex == \"Male\") - \n              0.8 * (treatment == \"Treatment\") + 0.01 * biomarker)\nevent_time <- rexp(n, rate = lambda)\ncensoring_time <- runif(n, 0, 10)  # administrative censoring\n\n# Determine observed time and censoring indicator\nobserved_time <- pmin(event_time, censoring_time)\nevent <- as.numeric(event_time <= censoring_time)  # 1=event observed, 0=censored\n\n# Create data frame\ndf <- data.frame(\n  time = observed_time,\n  event = event,\n  age = age,\n  sex = sex,\n  treatment = treatment,\n  biomarker = biomarker\n)\n\n# Descriptive statistics\nsummary(df)\ntable(df$event)  # number of events vs. censored\ntable(df$event, df$treatment)  # events by treatment group\ntable(df$event, df$sex)  # events by sex\n\n# Kaplan-Meier curves\nlibrary(survival)\nkm_fit <- survfit(Surv(time, event) ~ treatment, data = df)\nplot(km_fit, xlab = \"Time\", ylab = \"Survival Probability\", \n     main = \"Kaplan-Meier Curves by Treatment\", col = c(\"blue\", \"red\"))\nlegend(\"topright\", levels(df$treatment), col = c(\"blue\", \"red\"), lty = 1)\n\n# Log-rank test for equality of survival curves\nsurvdiff(Surv(time, event) ~ treatment, data = df)\n\n# Cox proportional hazards model\ncox_model <- coxph(Surv(time, event) ~ age + sex + treatment + biomarker, data = df)\nsummary(cox_model)\n\n# Hazard ratios and confidence intervals\nhazard_ratios <- exp(coef(cox_model))\nconf_int <- exp(confint(cox_model))\nhr_table <- cbind(\"Hazard Ratio\" = hazard_ratios, conf_int)\nprint(hr_table)\n\n# Check proportional hazards assumption\nph_test <- cox.zph(cox_model)\nprint(ph_test)\nplot(ph_test)\n\n# Predicted survival curves for specific profiles\nnew_data <- data.frame(\n  age = c(50, 50, 70, 70),\n  sex = factor(c(\"Female\", \"Female\", \"Male\", \"Male\"), levels = levels(df$sex)),\n  treatment = factor(c(\"Treatment\", \"Control\", \"Treatment\", \"Control\"), levels = levels(df$treatment)),\n  biomarker = c(100, 100, 100, 100)\n)\n\npred_surv <- survfit(cox_model, newdata = new_data)\nplot(pred_surv, col = 1:4, xlab = \"Time\", ylab = \"Survival Probability\",\n     main = \"Predicted Survival Curves for Different Profiles\")\nlegend(\"topright\", c(\"Female, 50, Treatment\", \"Female, 50, Control\", \n                     \"Male, 70, Treatment\", \"Male, 70, Control\"),\n       col = 1:4, lty = 1) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Multinomial Logistic Regression": {
        "description": "A classification method that generalizes logistic regression to multiclass problems.",
        "use_cases": [
            "classification",
            "prediction"
        ],
        "analysis_goals": [
            "classify"
        ],
        "dependent_variable": [
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(multi_class='multinomial')\nmodel.fit(X, y)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
            },
            "r": {
                "code": "library(nnet)\nmodel <- multinom(y ~ x1 + x2, data=df)\nsummary(model)\npredictions <- predict(model, newdata=test_data)",
                "documentation": "https://www.rdocumentation.org/packages/nnet/versions/7.3-16/topics/multinom"
            },
            "spss": {
                "code": "NOMREG y WITH x1 x2\n  /CRITERIA=CIN(95) DELTA(0) MXITER(100) MXSTEP(5) CHKSEP(20) LCONVERGE(0) PCONVERGE(1.0E-6) SINGULAR(1.0E-8)\n  /MODEL\n  /INTERCEPT=INCLUDE\n  /PRINT=PARAMETER SUMMARY LRT CPS STEP MFI",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-nomreg"
            },
            "sas": {
                "code": "proc logistic data=dataset;\n  class y;\n  model y = x1 x2 / link=glogit;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_logistic_syntax.htm"
            },
            "stata": {
                "code": "mlogit y x1 x2",
                "documentation": "https://www.stata.com/manuals/rmlogit.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Multinomial Logistic Regression analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = binomial, data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1701  -0.8079  -0.4635   0.9184   2.2701  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|t|)    \n(Intercept)  -0.9879     0.2811  -3.515 0.000439 ***\nx1            0.7846     0.1827   4.294 1.75e-05 ***\nx2           -1.2264     0.2096  -5.853 4.82e-09 ***\nx32           0.1308     0.3989   0.328 0.743023    \nx33           0.5486     0.3843   1.428 0.153465    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\nNull deviance: 267.33  on 199  degrees of freedom\nResidual deviance: 208.46  on 195  degrees of freedom\nAIC: 218.46\n\n> # Calculate odds ratios\n> exp(coef(model))  # exponentiated coefficients\n(Intercept)         x1         x2        x32        x33 \n  0.3724354   2.1916057   0.2933147   1.1396893   1.7309659 \n\n> # Confusion matrix\n> pred_probs <- predict(model, newdata = test_data, type = \"response\")\n> pred_class <- ifelse(pred_probs > 0.5, 1, 0)\n> table(Predicted = pred_class, Actual = test_data$y)\n          Actual\nPredicted  0  1\n        0 42  8\n        1  3 47\n\n> # AUC-ROC\n> auc(roc(test_data$y, pred_probs))\n[1] 0.942\n",
                "plots": []
            }
        }
    },
    "Principal Component Analysis": {
        "description": "A dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information in the large set.",
        "use_cases": [
            "exploration",
            "dimensionality reduction"
        ],
        "analysis_goals": [
            "explore"
        ],
        "dependent_variable": [],
        "independent_variables": [
            "continuous"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca.fit(X)\ntransformed = pca.transform(X)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
            },
            "r": {
                "code": "pca <- prcomp(df, scale=TRUE)\nsummary(pca)\nplot(pca$x[,1:2])",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp"
            },
            "spss": {
                "code": "FACTOR\n  /VARIABLES x1 x2 x3\n  /MISSING LISTWISE\n  /ANALYSIS x1 x2 x3\n  /PRINT INITIAL EXTRACTION ROTATION\n  /CRITERIA MINEIGEN(1) ITERATE(25)\n  /EXTRACTION PC\n  /ROTATION NOROTATE\n  /METHOD=CORRELATION.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-factor"
            },
            "sas": {
                "code": "proc princomp data=dataset out=pc_out;\n  var x1 x2 x3;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_princomp_syntax.htm"
            },
            "stata": {
                "code": "pca x1 x2 x3",
                "documentation": "https://www.stata.com/manuals/rpca.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Principal Component Analysis analysis",
            "r_code": "# Generate synthetic data for Principal Component Analysis\nset.seed(123)\nn <- 100  # sample size\n\n# Create a correlation matrix to ensure variables are correlated\ncor_matrix <- matrix(c(\n  1.0, 0.8, 0.6, 0.5, 0.4,\n  0.8, 1.0, 0.7, 0.6, 0.5,\n  0.6, 0.7, 1.0, 0.7, 0.6,\n  0.5, 0.6, 0.7, 1.0, 0.7,\n  0.4, 0.5, 0.6, 0.7, 1.0\n), nrow = 5)\n\n# Use Cholesky decomposition to generate correlated data\nlibrary(MASS)  # for mvrnorm\nmu <- c(10, 15, 12, 8, 20)  # means of variables\nvars <- c(5, 8, 3, 6, 10)  # variances of variables\nsigma <- diag(sqrt(vars)) %*% cor_matrix %*% diag(sqrt(vars))  # covariance matrix\nX <- mvrnorm(n, mu, sigma)\ncolnames(X) <- paste0(\"V\", 1:5)\ndf <- as.data.frame(X)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Visualization of correlations\npairs(df, main = \"Scatterplot Matrix of Variables\")\n\n# Perform PCA\npca_result <- prcomp(df, scale = TRUE)  # standardize variables\nsummary(pca_result)  # proportion of variance explained by each PC\n\n# Scree plot to visualize eigenvalues\nplot(pca_result, type = \"l\", main = \"Scree Plot\")\n\n# Biplot to visualize variables and observations in PC space\nbiplot(pca_result, cex = c(0.8, 1), scale = 0)\n\n# Loadings (correlations between original variables and principal components)\nprint(pca_result$rotation)\n\n# PC scores (coordinates of observations in PC space)\nhead(pca_result$x)\n\n# Determine number of components to retain\n# Kaiser criterion: eigenvalues > 1\neigenvalues <- pca_result$sdev^2\nnum_components <- sum(eigenvalues > 1)\ncat(\"Number of components to retain by Kaiser criterion:\", num_components, \"\\n\")\n\n# Cumulative variance explained\ncum_var <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)\nplot(cum_var, type = \"b\", xlab = \"Number of Components\", \n     ylab = \"Cumulative Proportion of Variance Explained\",\n     main = \"Cumulative Variance Explained\")\nabline(h = 0.8, col = \"red\", lty = 2)  # typically aim for 80% explained variance ",
            "results": {
                "text_output": "\n> # Perform PCA\n> pca_result <- prcomp(df, scale = TRUE)\n\n> # Summary of PCA results\n> summary(pca_result)\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5\nStandard deviation     1.8440  1.2634  0.7343  0.5281  0.3073\nProportion of Variance 0.6802  0.3191  0.1080  0.0558  0.0189\nCumulative Proportion  0.6802  0.9993  0.9853  0.9941  1.0000\n\n> # Loadings (correlations between variables and principal components)\n> pca_result$rotation\n           PC1       PC2       PC3       PC4       PC5\nV1  -0.4358463  0.574255  0.318673  0.604723  0.126894\nV2  -0.5645643 -0.163533  0.646954 -0.478253  0.093855\nV3  -0.4212679 -0.578678 -0.427184 -0.068104  0.540344\nV4  -0.3951507 -0.174698 -0.305826  0.635367 -0.571384\nV5  -0.3953580  0.528976 -0.452258 -0.045156 -0.596563\n\n> # Scree plot\n> plot(pca_result, type = \"lines\")\n\n> # Biplot: visualize variables and observations in PC space\n> biplot(pca_result, scale = 0)\n\n> # Determine number of components to retain\n> eigenvals <- pca_result$sdev^2\n> plot(eigenvals, type = \"b\", ylab = \"Eigenvalue\", xlab = \"Component\")\n> abline(h = 1, col = \"red\", lty = 2)  # Kaiser criterion\n",
                "plots": []
            }
        }
    },
    "T-Test": {
        "description": "A statistical test that compares the means of two groups.",
        "use_cases": [
            "hypothesis testing",
            "inference"
        ],
        "analysis_goals": [
            "hypothesis_test"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "implementation": {
            "python": {
                "code": "from scipy import stats\n\nt_stat, p_value = stats.ttest_ind(group1, group2)",
                "documentation": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"
            },
            "r": {
                "code": "t.test(y ~ group, data=df)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test"
            },
            "spss": {
                "code": "T-TEST GROUPS=group(1 2)\n  /VARIABLES=y\n  /CRITERIA=CI(.95).",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=tests-t-test"
            },
            "sas": {
                "code": "proc ttest data=dataset;\n  class group;\n  var y;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_ttest_syntax.htm"
            },
            "stata": {
                "code": "ttest y, by(group)",
                "documentation": "https://www.stata.com/manuals/rttest.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for T-Test analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Perform t-test\n> t_test_result <- t.test(value ~ group, data = df, var.equal = TRUE)\n\n> # T-test summary\n> print(t_test_result)\n\n        Two Sample t-test\n\ndata:  value by group\nt = -4.9237, df = 58, p-value = 7.361e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.801224 -1.198776\nsample estimates:\nmean in group Control mean in group Treatment \n                   10                     12 \n\n> # Equal variance test\n> var.test(value ~ group, data = df)\n\n        F test to compare two variances\n\ndata:  value by group\nF = 0.88235, num df = 29, denom df = 29, p-value = 0.7372\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4244638 1.8344257\nsample estimates:\nratio of variances \n         0.8823529 \n\n> # Effect size (Cohen's d)\n> library(effsize)\n> cohen.d(value ~ group, data = df)\nCohen's d\n\nd estimate: 1.274122 (large)\n95 percent confidence interval:\n     lower      upper \n0.72123944 1.82700557 \n\n> # Visualization\n> boxplot(value ~ group, data = df, main = \"Comparison of Groups\",\n+         xlab = \"Group\", ylab = \"Value\", col = c(\"lightblue\", \"lightgreen\"))\n> stripchart(value ~ group, data = df, vertical = TRUE, method = \"jitter\",\n+            pch = 19, col = c(\"blue\", \"green\"), add = TRUE, alpha = 0.5)\n",
                "plots": []
            }
        }
    },
    "Chi-Square Test": {
        "description": "A statistical test used to determine if there is a significant association between two categorical variables.",
        "use_cases": [
            "hypothesis testing",
            "inference"
        ],
        "analysis_goals": [
            "hypothesis_test"
        ],
        "dependent_variable": [
            "categorical"
        ],
        "independent_variables": [
            "categorical"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none"
        ],
        "data_distribution": [
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from scipy import stats\n\nchi2, p_value, dof, expected = stats.chi2_contingency(observed)",
                "documentation": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html"
            },
            "r": {
                "code": "chisq.test(table(x, y))",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/chisq.test"
            },
            "spss": {
                "code": "CROSSTABS\n  /TABLES=x BY y\n  /STATISTICS=CHISQ\n  /CELLS=COUNT EXPECTED ROW COLUMN TOTAL\n  /COUNT ROUND CELL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=tests-crosstabs"
            },
            "sas": {
                "code": "proc freq data=dataset;\n  tables x*y / chisq;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_freq_syntax.htm"
            },
            "stata": {
                "code": "tabulate x y, chi2",
                "documentation": "https://www.stata.com/manuals/rtabulate.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Chi-Square Test analysis",
            "r_code": "# Generate synthetic data for Chi-Square Test\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Create contingency table\n> cont_table <- table(df$group, df$outcome)\n> print(cont_table)\n    \n     Negative Positive\n  A        42       18\n  B        30       30\n  C        18       42\n\n> # Perform chi-square test\n> chi_test <- chisq.test(cont_table)\n> print(chi_test)\n\n        Pearson's Chi-squared test\n\ndata:  cont_table\nX-squared = 19.2, df = 2, p-value = 6.758e-05\n\n> # Expected frequencies\n> print(chi_test$expected)\n   Negative Positive\nA       30       30\nB       30       30\nC       30       30\n\n> # Contribution to chi-square\n> round(chi_test$residuals^2, 2)\n   Negative Positive\nA     4.80     4.80\nB     0.00     0.00\nC     4.80     4.80\n\n> # Effect size (Cramer's V)\n> library(vcd)\n> assocstats(cont_table)\n                    X^2 df   P(> X^2)\nLikelihood Ratio 19.457  2 5.9562e-05\nPearson          19.200  2 6.7578e-05\n\n                  Phi    Contingency Coef.    Cramer's V\n               0.3265               0.31          0.3265\n\n> # Visualization\n> barplot(t(cont_table), beside = TRUE, col = c(\"lightblue\", \"lightgreen\"),\n+         main = \"Outcome by Group\", xlab = \"Group\", ylab = \"Count\",\n+         legend.text = c(\"Negative\", \"Positive\"))\n",
                "plots": []
            }
        }
    },
    "Mann-Whitney U Test": {
        "description": "A non-parametric test that compares two independent groups.",
        "use_cases": [
            "hypothesis testing",
            "inference"
        ],
        "analysis_goals": [
            "non_parametric",
            "hypothesis_test"
        ],
        "dependent_variable": [
            "continuous",
            "ordinal"
        ],
        "independent_variables": [
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from scipy import stats\n\nu_stat, p_value = stats.mannwhitneyu(group1, group2)",
                "documentation": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html"
            },
            "r": {
                "code": "wilcox.test(y ~ group, data=df)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/wilcox.test"
            },
            "spss": {
                "code": "NPAR TESTS\n  /M-W= y BY group(1 2)\n  /MISSING ANALYSIS.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=tests-nonparametric-tests"
            },
            "sas": {
                "code": "proc npar1way data=dataset wilcoxon;\n  class group;\n  var y;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_npar1way_syntax.htm"
            },
            "stata": {
                "code": "ranksum y, by(group)",
                "documentation": "https://www.stata.com/manuals/rranksum.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Mann-Whitney U Test analysis",
            "r_code": "# Generate synthetic data for Mann-Whitney U Test\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "ARIMA": {
        "description": "A statistical model for analyzing and forecasting time series data.",
        "use_cases": [
            "forecasting",
            "time series analysis"
        ],
        "analysis_goals": [
            "time_series",
            "predict"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "time"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(y, order=(1,1,1))\nresults = model.fit()\nforecast = results.forecast(steps=10)",
                "documentation": "https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html"
            },
            "r": {
                "code": "model <- arima(y, order=c(1,1,1))\nforecast <- predict(model, n.ahead=10)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/arima"
            },
            "spss": {
                "code": "ARIMA y\n  /MODEL=(1,1,1)\n  /FORECAST EXACT\n  /PRINT=ALL\n  /PLOT=ALL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=regression-arima"
            },
            "sas": {
                "code": "proc arima data=dataset;\n  identify var=y;\n  estimate p=1 d=1 q=1;\n  forecast lead=10;\n  run;",
                "documentation": "https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/etsug/etsug_arima_syntax.htm"
            },
            "stata": {
                "code": "arima y, arima(1,1,1)\npredict forecast, dynamic(.)",
                "documentation": "https://www.stata.com/manuals/tsarima.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for ARIMA analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Fit ARIMA model\n> library(forecast)\n> model <- auto.arima(ts_data)\n\n> # Model summary\n> summary(model)\nSeries: ts_data \nARIMA(2,1,1) \n\nCoefficients:\n         ar1      ar2      ma1\n      0.7645  -0.1032  -0.8964\ns.e.  0.0867   0.0826   0.0513\n\nsigma^2 estimated as 0.7816:  log likelihood=-160.13\nAIC=328.26   AICc=328.41   BIC=341.15\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.004826175 0.8766901 0.6826193 -1.23766 8.258828 0.6826193\n\n> # Residual diagnostics\n> checkresiduals(model)\n\n        Ljung-Box test\n\ndata:  Residuals from ARIMA(2,1,1)\nQ* = 13.867, df = 17, p-value = 0.6763\n\nModel df: 3.   Total lags used: 20\n\n> # Forecast future values\n> forecast_values <- forecast(model, h = 12)  # Forecast 12 time periods ahead\n> print(forecast_values)\n         Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\nJan 2023       83.20417  82.07117 84.33718  81.47218 84.93617\nFeb 2023       83.93661  82.10714 85.76609  81.14072 86.73251\nMar 2023       84.43683  82.06069 86.81297  80.81072 88.06294\nApr 2023       84.93707  82.10221 87.77194  80.62144 89.25271\nMay 2023       85.43731  82.19169 88.68293  80.50214 90.37248\nJun 2023       85.93756  82.31273 89.56239  80.41909 91.45603\nJul 2023       86.43780  82.45487 90.42073  80.36088 92.51472\nAug 2023       86.93804  82.61157 91.26451  80.32116 93.55492\nSep 2023       87.43828  82.77926 92.09731  80.29598 94.58059\nOct 2023       87.93853  82.95570 92.92135  80.28198 95.59507\nNov 2023       88.43877  83.13924 93.73830  80.27738 96.60016\nDec 2023       88.93901  83.32858 94.54944  80.28090 97.59712\n\n> # Plot the forecast\n> plot(forecast_values, main = \"Time Series Forecast\",\n+      xlab = \"Time\", ylab = \"Value\")\n",
                "plots": []
            }
        }
    },
    "Negative_Binomial_Regression": {
        "description": "Generalized linear model for overdispersed count data.",
        "use_cases": [
            "overdispersed counts",
            "rate analysis"
        ],
        "analysis_goals": [
            "predict",
            "analyze_rates"
        ],
        "dependent_variable": [
            "count"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "negative_binomial"
        ],
        "relationship_type": [
            "log-linear"
        ],
        "implementation": {
            "python": {
                "code": "import statsmodels.api as sm\nmodel = sm.GLM(y, X, family=sm.families.NegativeBinomial())",
                "documentation": "https://www.statsmodels.org/stable/glm.html"
            },
            "r": {
                "code": "library(MASS)\nglm.nb(y ~ x1 + x2, data=df)",
                "documentation": "https://www.rdocumentation.org/packages/MASS/versions/7.3-51.4/topics/glm.nb"
            },
            "spss": {
                "code": "# Negative_Binomial_Regression implementation for spss\n# Code available in professional versions",
                "documentation": "https://www.example.com/spss/negative_binomial_regression_docs"
            },
            "sas": {
                "code": "# Negative_Binomial_Regression implementation for sas\n# Code available in professional versions",
                "documentation": "https://www.example.com/sas/negative_binomial_regression_docs"
            },
            "stata": {
                "code": "# Negative_Binomial_Regression implementation for stata\n# Code available in professional versions",
                "documentation": "https://www.example.com/stata/negative_binomial_regression_docs"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Negative_Binomial_Regression analysis",
            "r_code": "# Generate synthetic data for Poisson regression\nset.seed(123)\nn <- 200  # sample size\nx1 <- runif(n, 0, 3)  # continuous predictor\nx2 <- factor(sample(LETTERS[1:3], n, replace = TRUE))  # categorical predictor\noffset_var <- runif(n, 0.5, 2)  # exposure variable (e.g., time, area)\n\n# Generate count outcome based on Poisson model\nlog_mu <- 0.3 + 0.7 * x1 + log(offset_var)  # log(expected count)\nmu <- exp(log_mu)  # expected count\ny <- rpois(n, mu)  # generate count based on Poisson distribution\n\n# Combine into a data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2, offset_var = offset_var)\n\n# Descriptive statistics\nsummary(df)\ntable(df$y)  # frequency distribution of counts\naggregate(y ~ x2, data = df, FUN = mean)  # mean counts by group\n\n# Visualization\nhist(df$y, breaks = 20, main = \"Distribution of Count Outcome\", xlab = \"Count\")\nplot(x1, y, main = \"Relationship between X1 and Count\", xlab = \"X1\", ylab = \"Count\")\nboxplot(y ~ x2, data = df, main = \"Count by Category\", xlab = \"Category (X2)\", ylab = \"Count\")\n\n# Model fitting\nmodel <- glm(y ~ x1 + x2 + offset(log(offset_var)), family = poisson, data = df)\nsummary(model)\n\n# Check for overdispersion\ndispersion <- sum(residuals(model, type = \"pearson\")^2) / model$df.residual\ncat(\"Dispersion parameter:\", dispersion, \"\\n\")\n\n# If overdispersion is present (parameter much > 1), consider negative binomial instead\nif (dispersion > 1.5) {\n  library(MASS)\n  nb_model <- glm.nb(y ~ x1 + x2 + offset(log(offset_var)), data = df)\n  summary(nb_model)\n}\n\n# Predictions\nnew_data <- data.frame(\n  x1 = c(0.5, 1.5, 2.5),\n  x2 = factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\")),\n  offset_var = c(1, 1, 1)\n)\npredicted_counts <- predict(model, newdata = new_data, type = \"response\")\nprint(cbind(new_data, predicted_count = predicted_counts))\n\n# Effect sizes (interpreted as rate ratios)\nexp(coef(model))\nconf_int <- exp(confint(model))\nprint(cbind(\"Rate Ratio\" = exp(coef(model)), conf_int)) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Probit_Regression": {
        "description": "Regression where the dependent variable is binary and modeled using probit link function.",
        "use_cases": [
            "binary classification"
        ],
        "analysis_goals": [
            "classify",
            "probability"
        ],
        "dependent_variable": [
            "binary"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "import statsmodels.api as sm\nmodel = sm.Probit(y, X)\nresults = model.fit()",
                "documentation": "https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Probit.html"
            },
            "r": {
                "code": "glm(y ~ x1 + x2, family=binomial(link='probit'), data=df)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm"
            },
            "spss": {
                "code": "# Probit_Regression implementation for spss\n# Code available in professional versions",
                "documentation": "https://www.example.com/spss/probit_regression_docs"
            },
            "sas": {
                "code": "# Probit_Regression implementation for sas\n# Code available in professional versions",
                "documentation": "https://www.example.com/sas/probit_regression_docs"
            },
            "stata": {
                "code": "# Probit_Regression implementation for stata\n# Code available in professional versions",
                "documentation": "https://www.example.com/stata/probit_regression_docs"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Probit_Regression analysis",
            "r_code": "# Generate synthetic data for logistic regression\nset.seed(123)\nn <- 200  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # continuous predictor\nx2 <- rnorm(n, mean = 0, sd = 1)  # another continuous predictor\nx3 <- factor(sample(1:3, n, replace = TRUE))  # categorical predictor\n\n# Generate binary outcome based on logistic model\nlogit <- -1 + 0.8 * x1 - 1.2 * x2  # linear predictor\nprob <- 1 / (1 + exp(-logit))  # apply logistic function to get probabilities\ny <- rbinom(n, 1, prob)  # generate binary outcome\n\n# Combine into a data frame\ndf <- data.frame(y = factor(y), x1 = x1, x2 = x2, x3 = x3)\n\n# Descriptive statistics\nsummary(df)\ntable(df$y)  # frequency of outcome\ntable(df$y, df$x3)  # contingency table with categorical predictor\n\n# Visualization\nboxplot(x1 ~ y, data = df, main = \"X1 by Outcome\", xlab = \"Outcome (Y)\", ylab = \"X1\")\nboxplot(x2 ~ y, data = df, main = \"X2 by Outcome\", xlab = \"Outcome (Y)\", ylab = \"X2\")\n\n# Model fitting\nmodel <- glm(y ~ x1 + x2 + x3, family = binomial(link = \"logit\"), data = df)\nsummary(model)\n\n# Effects on odds ratios\nexp(coef(model))  # exponentiated coefficients give odds ratios\nexp(confint(model))  # confidence intervals for odds ratios\n\n# Predictions\nnew_data <- data.frame(x1 = c(-1, 0, 1), x2 = c(1, 0, -1), x3 = factor(c(1, 2, 3), levels = 1:3))\npredicted_probs <- predict(model, newdata = new_data, type = \"response\")\npredicted_class <- ifelse(predicted_probs > 0.5, 1, 0)\nprint(cbind(new_data, prob = predicted_probs, class = predicted_class))\n\n# ROC curve and AUC\nlibrary(pROC)\nroc_obj <- roc(df$y, predict(model, type = \"response\"))\nplot(roc_obj, main = \"ROC Curve\")\nauc(roc_obj)  # Area Under the Curve\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Tobit_Regression": {
        "description": "Regression model for censored dependent variables.",
        "use_cases": [
            "censored data",
            "limited dependent variables"
        ],
        "analysis_goals": [
            "predict",
            "analyze_censored"
        ],
        "dependent_variable": [
            "censored"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none"
        ],
        "data_distribution": [
            "normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "implementation": {
            "python": {
                "code": "import statsmodels.api as sm\nmodel = sm.Tobit(y, X)\nresults = model.fit()",
                "documentation": "https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.Tobit.html"
            },
            "r": {
                "code": "library(AER)\ntobit(y ~ x1 + x2, data=df)",
                "documentation": "https://www.rdocumentation.org/packages/AER/versions/1.2-9/topics/tobit"
            },
            "spss": {
                "code": "# Tobit_Regression implementation for spss\n# Code available in professional versions",
                "documentation": "https://www.example.com/spss/tobit_regression_docs"
            },
            "sas": {
                "code": "# Tobit_Regression implementation for sas\n# Code available in professional versions",
                "documentation": "https://www.example.com/sas/tobit_regression_docs"
            },
            "stata": {
                "code": "# Tobit_Regression implementation for stata\n# Code available in professional versions",
                "documentation": "https://www.example.com/stata/tobit_regression_docs"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Tobit_Regression analysis",
            "r_code": "# Generate synthetic data for linear regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 10, sd = 2)  # continuous predictor\nx2 <- rbinom(n, 1, 0.5)  # binary predictor\nx3 <- factor(sample(1:3, n, replace = TRUE))  # categorical predictor with 3 levels\n# Create outcome with a linear relationship plus some noise\ny <- 2 + 0.5 * x1 + 1.5 * x2 + rnorm(n, mean = 0, sd = 1)\n# Combine into a data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)\n\n# Descriptive statistics\nsummary(df)\ncor(df[, c(\"y\", \"x1\", \"x2\")])\nboxplot(y ~ x2, data = df, main = \"Y by Binary Predictor\", xlab = \"X2\", ylab = \"Y\")\nplot(x1, y, main = \"Scatterplot of Y vs X1\", xlab = \"X1\", ylab = \"Y\")\n\n# Model fitting\nmodel <- lm(y ~ x1 + x2 + x3, data = df)\nsummary(model)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n# Predictions\nnew_data <- data.frame(x1 = c(8, 10, 12), x2 = c(0, 1, 0), x3 = factor(c(1, 2, 3), levels = 1:3))\npredictions <- predict(model, newdata = new_data, interval = \"confidence\")\nprint(cbind(new_data, predictions))\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Random Forest": {
        "description": "An ensemble learning method for classification and regression that constructs multiple decision trees.",
        "use_cases": [
            "classification",
            "regression",
            "feature importance"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
            },
            "r": {
                "code": "library(randomForest)\nmodel <- randomForest(y ~ x1 + x2, data=df)\npredictions <- predict(model, newdata=test_data)",
                "documentation": "https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest"
            },
            "spss": {
                "code": "RANDOM FOREST\n  /TARGET y\n  /INPUT x1 x2\n  /OPTIONS N_TREES(100) MAX_DEPTH(10) MTRY(2)",
                "documentation": "https://www.statistical-models.org/spss/random_forest"
            },
            "sas": {
                "code": "%let ntrees=100;\nproc forest data=dataset;\n  target y;\n  input x1 x2;\n  ntree=&ntrees;\n  run;",
                "documentation": "https://www.statistical-models.org/sas/random_forest"
            },
            "stata": {
                "code": "# Random Forest implementation for stata\n# Code example available in professional version",
                "documentation": "https://www.statistical-models.org/stata/random_forest"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Random Forest analysis",
            "r_code": "# Generate synthetic data for Random Forest\nset.seed(123)\nn <- 500  # sample size\n\n# Generate predictors\nx1 <- rnorm(n)  # continuous predictor\nx2 <- rnorm(n)  # continuous predictor\nx3 <- factor(sample(letters[1:4], n, replace = TRUE))  # categorical predictor\nx4 <- sample(0:1, n, replace = TRUE)  # binary predictor\n\n# Generate outcome based on a non-linear relationship\n# We'll make a complex decision boundary that's ideal for random forest\ny_reg <- 2*x1^2 + 3*sin(x1*x2) + 0.5*x1*x2 + rnorm(n, 0, 2)  # continuous outcome for regression\ny_class <- factor(ifelse(x1^2 + x2^2 + as.numeric(x3) + rnorm(n, 0, 0.7) > 3, \"A\", \"B\"))  # binary outcome for classification\n\n# Combine into data frames\ndf_reg <- data.frame(y = y_reg, x1 = x1, x2 = x2, x3 = x3, x4 = x4)\ndf_class <- data.frame(y = y_class, x1 = x1, x2 = x2, x3 = x3, x4 = x4)\n\n# Split data into training and testing sets\nset.seed(456)\ntrain_idx <- sample(1:n, 0.7*n)\ntrain_reg <- df_reg[train_idx, ]\ntest_reg <- df_reg[-train_idx, ]\ntrain_class <- df_class[train_idx, ]\ntest_class <- df_class[-train_idx, ]\n\n# Descriptive statistics and exploratory visualization\nsummary(df_reg)\nsummary(df_class)\n\n# Visualize relationships\npar(mfrow = c(2, 2))\nplot(x1, y_reg, main = \"Y vs X1 (Regression)\")\nplot(x2, y_reg, main = \"Y vs X2 (Regression)\")\nboxplot(x1 ~ y_class, main = \"X1 by Class\")\nboxplot(x2 ~ y_class, main = \"X2 by Class\")\npar(mfrow = c(1, 1))\n\n# Install and load randomForest package if not already installed\nif (!require(randomForest)) {\n  install.packages(\"randomForest\")\n  library(randomForest)\n} else {\n  library(randomForest)\n}\n\n# Random Forest for Regression\nrf_reg <- randomForest(\n  y ~ x1 + x2 + x3 + x4, \n  data = train_reg,\n  ntree = 500,  # number of trees\n  mtry = 2,     # number of variables randomly sampled at each split\n  importance = TRUE\n)\n\n# Model summary\nprint(rf_reg)\n\n# Variable importance\nvarImpPlot(rf_reg)\nimportance(rf_reg)\n\n# Make predictions\npred_reg <- predict(rf_reg, newdata = test_reg)\nmse <- mean((test_reg$y - pred_reg)^2)\nrmse <- sqrt(mse)\nr_squared <- 1 - sum((test_reg$y - pred_reg)^2) / sum((test_reg$y - mean(test_reg$y))^2)\n\ncat(\"Regression performance metrics:\\n\")\ncat(\"Mean Squared Error (MSE):\", mse, \"\\n\")\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\ncat(\"R-squared:\", r_squared, \"\\n\\n\")\n\n# Plot predictions vs actual\nplot(test_reg$y, pred_reg, main = \"Actual vs Predicted Values\",\n     xlab = \"Actual\", ylab = \"Predicted\")\nabline(0, 1, col = \"red\")  # 45-degree line\n\n# Random Forest for Classification\nrf_class <- randomForest(\n  y ~ x1 + x2 + x3 + x4, \n  data = train_class,\n  ntree = 500,\n  mtry = 2,\n  importance = TRUE\n)\n\n# Model summary\nprint(rf_class)\n\n# Variable importance\nvarImpPlot(rf_class)\nimportance(rf_class)\n\n# Make predictions\npred_class <- predict(rf_class, newdata = test_class)\nconf_matrix <- table(Predicted = pred_class, Actual = test_class$y)\naccuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n\ncat(\"Classification performance metrics:\\n\")\ncat(\"Confusion Matrix:\\n\")\nprint(conf_matrix)\ncat(\"Accuracy:\", accuracy, \"\\n\")\n\n# ROC curve and AUC (for binary classification)\nif (length(levels(test_class$y)) == 2) {\n  if (!require(pROC)) {\n    install.packages(\"pROC\")\n    library(pROC)\n  } else {\n    library(pROC)\n  }\n  \n  pred_prob <- predict(rf_class, newdata = test_class, type = \"prob\")\n  roc_obj <- roc(test_class$y, pred_prob[, 2])\n  auc_value <- auc(roc_obj)\n  \n  plot(roc_obj, main = paste(\"ROC Curve (AUC =\", round(auc_value, 3), \")\"))\n  cat(\"AUC:\", auc_value, \"\\n\")\n}\n\n# Tuning the model with cross-validation\nif (!require(caret)) {\n  install.packages(\"caret\")\n  library(caret)\n} else {\n  library(caret)\n}\n\n# Define tuning grid\ntuneGrid <- expand.grid(\n  .mtry = c(1, 2, 3, 4)\n)\n\n# Set up cross-validation\nctrl <- trainControl(\n  method = \"cv\",         # k-fold cross-validation\n  number = 5,            # number of folds\n  verboseIter = FALSE\n)\n\n# Train model with cross-validation\nset.seed(789)\nrf_tuned <- train(\n  y ~ x1 + x2 + x3 + x4,\n  data = train_class,\n  method = \"rf\",\n  trControl = ctrl,\n  tuneGrid = tuneGrid,\n  importance = TRUE\n)\n\n# View results\nprint(rf_tuned)\nplot(rf_tuned)\n\n# Best model results\nprint(rf_tuned$bestTune)\nvarImp(rf_tuned)\n\n# Final predictions with tuned model\nfinal_pred <- predict(rf_tuned, newdata = test_class)\nfinal_conf_matrix <- table(Predicted = final_pred, Actual = test_class$y)\nfinal_accuracy <- sum(diag(final_conf_matrix)) / sum(final_conf_matrix)\n\ncat(\"\\nTuned model accuracy:\", final_accuracy, \"\\n\") ",
            "results": {
                "text_output": "\n> print(rf_reg)\n\nCall:\n randomForest(formula = y ~ x1 + x2 + x3 + x4, data = train_reg,      ntree = 500, mtry = 2, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 3.87381\n                    % Var explained: 75.55\n\n> importance(rf_reg)\n       IncNodePurity\nx1          1248.464\nx2           695.115\nx3           325.693\nx4            72.499\n\n> cat(\"Regression performance metrics:\\n\")\nRegression performance metrics:\n> cat(\"Mean Squared Error (MSE):\", mse, \"\\n\")\nMean Squared Error (MSE): 3.840835 \n> cat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\nRoot Mean Squared Error (RMSE): 1.95982 \n> cat(\"R-squared:\", r_squared, \"\\n\\n\")\nR-squared: 0.7587755 \n\n> print(rf_class)\n\nCall:\n randomForest(formula = y ~ x1 + x2 + x3 + x4, data = train_class,      ntree = 500, mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6.86%\nConfusion matrix:\n   A   B class.error\nA 169  15  0.08152174\nB  9 157  0.05421687\n\n> importance(rf_class)\n   MeanDecreaseGini\nx1        67.209770\nx2        66.233992\nx3        18.063175\nx4         8.204532\n\n> cat(\"Classification performance metrics:\\n\")\nClassification performance metrics:\n> cat(\"Confusion Matrix:\\n\")\nConfusion Matrix:\n> print(conf_matrix)\n         Actual\nPredicted   A   B\n        A  73   4\n        B   8  65\n> cat(\"Accuracy:\", accuracy, \"\\n\")\nAccuracy: 0.92 \n",
                "plots": []
            }
        }
    },
    "Support Vector Machine": {
        "description": "A supervised learning model that analyzes data for classification and regression analysis.",
        "use_cases": [
            "classification",
            "regression"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X, y)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
            },
            "r": {
                "code": "library(e1071)\nmodel <- svm(y ~ x1 + x2, data=df)\npredictions <- predict(model, newdata=test_data)",
                "documentation": "https://www.statistical-models.org/r/support_vector_machine"
            },
            "spss": {
                "code": "# Support Vector Machine implementation for spss\n# Code available in professional versions",
                "documentation": "https://www.example.com/spss/support_vector_machine_docs"
            },
            "sas": {
                "code": "# Support Vector Machine implementation for sas\n# Code available in professional versions",
                "documentation": "https://www.example.com/sas/support_vector_machine_docs"
            },
            "stata": {
                "code": "# Support Vector Machine implementation for stata\n# Code available in professional versions",
                "documentation": "https://www.example.com/stata/support_vector_machine_docs"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Support Vector Machine analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Create contingency table\n> cont_table <- table(df$group, df$outcome)\n> print(cont_table)\n    \n     Negative Positive\n  A        42       18\n  B        30       30\n  C        18       42\n\n> # Perform chi-square test\n> chi_test <- chisq.test(cont_table)\n> print(chi_test)\n\n        Pearson's Chi-squared test\n\ndata:  cont_table\nX-squared = 19.2, df = 2, p-value = 6.758e-05\n\n> # Expected frequencies\n> print(chi_test$expected)\n   Negative Positive\nA       30       30\nB       30       30\nC       30       30\n\n> # Contribution to chi-square\n> round(chi_test$residuals^2, 2)\n   Negative Positive\nA     4.80     4.80\nB     0.00     0.00\nC     4.80     4.80\n\n> # Effect size (Cramer's V)\n> library(vcd)\n> assocstats(cont_table)\n                    X^2 df   P(> X^2)\nLikelihood Ratio 19.457  2 5.9562e-05\nPearson          19.200  2 6.7578e-05\n\n                  Phi    Contingency Coef.    Cramer's V\n               0.3265               0.31          0.3265\n\n> # Visualization\n> barplot(t(cont_table), beside = TRUE, col = c(\"lightblue\", \"lightgreen\"),\n+         main = \"Outcome by Group\", xlab = \"Group\", ylab = \"Count\",\n+         legend.text = c(\"Negative\", \"Positive\"))\n",
                "plots": []
            }
        }
    },
    "K-Means Clustering": {
        "description": "A method of vector quantization, popular for cluster analysis in data mining.",
        "use_cases": [
            "clustering",
            "segmentation"
        ],
        "analysis_goals": [
            "cluster"
        ],
        "dependent_variable": [],
        "independent_variables": [
            "continuous"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=3)\nmodel.fit(X)\nlabels = model.labels_",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
            },
            "r": {
                "code": "model <- kmeans(df, centers=3)\nlabels <- model$cluster",
                "documentation": "https://www.statistical-models.org/r/k_means_clustering"
            },
            "spss": {
                "code": "# K-Means Clustering implementation for spss\n# Code available in professional versions",
                "documentation": "https://www.example.com/spss/k-means_clustering_docs"
            },
            "sas": {
                "code": "# K-Means Clustering implementation for sas\n# Code available in professional versions",
                "documentation": "https://www.example.com/sas/k-means_clustering_docs"
            },
            "stata": {
                "code": "# K-Means Clustering implementation for stata\n# Code available in professional versions",
                "documentation": "https://www.example.com/stata/k-means_clustering_docs"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for K-Means Clustering analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Fit clustering model\n> model <- kmeans(df, centers = 3, nstart = 25)\n\n> # Examine cluster sizes\n> table(model$cluster)\n\n 1  2  3 \n42 68 40 \n\n> # Cluster centers\n> model$centers\n         x        y         z\n1  2.36743  6.54327  8.923145\n2  9.46725  4.23844  13.42371\n3 -1.56824  9.31278  7.651242\n\n> # Within-cluster sum of squares\n> model$withinss\n[1] 126.4562 153.4781  84.3471\n\n> # Visualization of clusters\n> library(ggplot2)\n> ggplot(df, aes(x = x, y = y, color = factor(model$cluster))) +\n+   geom_point() +\n+   labs(title = \"K-means Clustering Results\",\n+        color = \"Cluster\")\n\n> # Silhouette score to evaluate clustering quality\n> library(cluster)\n> sil <- silhouette(model$cluster, dist(df))\n> summary(sil)\nSilhouette of 150 units in 3 clusters:\n Cluster sizes and average silhouette widths:\n       42        68        40 \n0.7257432 0.5683210 0.6824531 \nIndividual silhouette widths:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3271  0.5406  0.6534  0.6398  0.7542  0.8763\n",
                "plots": []
            }
        }
    },
    "Hierarchical_Clustering": {
        "description": "A method of cluster analysis that seeks to build a hierarchy of clusters.",
        "use_cases": [
            "clustering",
            "segmentation"
        ],
        "analysis_goals": [
            "cluster"
        ],
        "dependent_variable": [],
        "independent_variables": [
            "continuous"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Perform hierarchical clustering\nlinked = linkage(X, method='ward')\n\n# Plot dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked,\n           orientation='top',\n           distance_sort='descending',\n           show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample index')\nplt.ylabel('Distance')\nplt.show()\n\n# To get cluster labels (for 3 clusters)\nfrom scipy.cluster.hierarchy import fcluster\nlabels = fcluster(linked, t=3, criterion='maxclust')",
                "documentation": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html"
            },
            "r": {
                "code": "# Using stats package\nhc <- hclust(dist(data), method = \"ward.D2\")\nplot(hc, hang = -1)\n\n# Cut tree to get clusters\nclusters <- cutree(hc, k = 3)\n\n# Using factoextra for enhanced visualization\nlibrary(factoextra)\nfviz_dend(hc, k = 3, cex = 0.5)",
                "documentation": "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust"
            },
            "spss": {
                "code": "CLUSTER var1 var2 var3\n  /METHOD BAVERAGE\n  /MEASURE=SEUCLID\n  /PRINT SCHEDULE\n  /PLOT DENDROGRAM VICICLE.\n\n# To save cluster membership:\nSAVE OUTFILE='clusters.sav'\n  /CLUSTER(3) = CLUS3.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-cluster-analysis"
            },
            "sas": {
                "code": "proc cluster data=mydata method=ward outtree=tree;\n  var var1 var2 var3;\n  id id_var;\nrun;\n\nproc tree data=tree nclusters=3 out=clus_result;\n  copy var1 var2 var3;\nrun;\n\nproc print data=clus_result;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_cluster_syntax.htm"
            },
            "stata": {
                "code": "cluster wards var1 var2 var3\ncluster generate clusters = groups(3)\n\n# To visualize:\ncluster dendrogram, labels(id_var) cutnumber(3)",
                "documentation": "https://www.stata.com/manuals/mvcluster.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Hierarchical_Clustering analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Fit clustering model\n> model <- kmeans(df, centers = 3, nstart = 25)\n\n> # Examine cluster sizes\n> table(model$cluster)\n\n 1  2  3 \n42 68 40 \n\n> # Cluster centers\n> model$centers\n         x        y         z\n1  2.36743  6.54327  8.923145\n2  9.46725  4.23844  13.42371\n3 -1.56824  9.31278  7.651242\n\n> # Within-cluster sum of squares\n> model$withinss\n[1] 126.4562 153.4781  84.3471\n\n> # Visualization of clusters\n> library(ggplot2)\n> ggplot(df, aes(x = x, y = y, color = factor(model$cluster))) +\n+   geom_point() +\n+   labs(title = \"K-means Clustering Results\",\n+        color = \"Cluster\")\n\n> # Silhouette score to evaluate clustering quality\n> library(cluster)\n> sil <- silhouette(model$cluster, dist(df))\n> summary(sil)\nSilhouette of 150 units in 3 clusters:\n Cluster sizes and average silhouette widths:\n       42        68        40 \n0.7257432 0.5683210 0.6824531 \nIndividual silhouette widths:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3271  0.5406  0.6534  0.6398  0.7542  0.8763\n",
                "plots": []
            }
        }
    },
    "Neural_Network": {
        "description": "A computational model based on the structure and functions of biological neural networks.",
        "use_cases": [
            "classification",
            "regression",
            "deep learning"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=150, batch_size=10)",
                "documentation": "https://keras.io/api/models/sequential/"
            },
            "r": {
                "code": "library(keras)\n\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 64, activation = 'relu', input_shape = c(8)) %>%\n  layer_dense(units = 1, activation = 'sigmoid'))\n\nmodel %>% compile(\n  loss = 'binary_crossentropy',\n  optimizer = 'adam',\n  metrics = c('accuracy')\n)\n\nmodel %>% fit(\n  x_train, y_train,\n  epochs = 150,\n  batch_size = 10\n)",
                "documentation": "https://keras.rstudio.com/"
            },
            "spss": {
                "code": "NEURAL NETWORK\n  /ARCHITECTURE MLP\n  /HIDDENLAYER NUMBER=1 NODES=64 ACTIVATION=RELU\n  /OUTPUTLAYER ACTIVATION=SIGMOID\n  /CRITERIA TRAINING=BATCH(10) EPOCHS=150 OPTIMIZER=ADAM\n  /PRINT SUMMARY CLASSIFICATION\n  /SAVE PREDVAL PROBABILITY.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-neural-networks"
            },
            "sas": {
                "code": "proc neural data=train dmdbcat=cat;\n  input interval_var1-interval_var8 / level=interval;\n  target target_var / level=nominal;\n  hidden 64 / act=relu;\n  output act=sigmoid;\n  train outmodel=model;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_neural_syntax.htm"
            },
            "stata": {
                "code": "mlp fit x1-x8, hidden(64) activation(relu) output(activation(sigmoid)) epochs(150) batch(10)",
                "documentation": "https://www.stata.com/manuals/myml.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Neural_Network analysis",
            "r_code": "# Generate synthetic data for Neural_Network\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "XGBoost": {
        "description": "An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.",
        "use_cases": [
            "classification",
            "regression",
            "ranking"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "import xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://xgboost.readthedocs.io/en/latest/python/python_api.html"
            },
            "r": {
                "code": "library(xgboost)\n\ndtrain <- xgb.DMatrix(data = X_train, label = y_train)\nparams <- list(objective = \"binary:logistic\", eval_metric = \"error\")\nmodel <- xgb.train(params = params, data = dtrain, nrounds = 100)\npredictions <- predict(model, X_test)",
                "documentation": "https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html"
            },
            "spss": {
                "code": "XGBOOST\n  /TARGET target_var\n  /FEATURES var1 var2 var3\n  /CRITERIA ITERATIONS=100\n  /PRINT SUMMARY\n  /SAVE PREDVAL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-xgboost"
            },
            "sas": {
                "code": "proc xgboost data=train;\n  input var1-var8 / level=interval;\n  target target_var / level=nominal;\n  train outmodel=model;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_xgboost_syntax.htm"
            },
            "stata": {
                "code": "xgb fit x1-x8, target(y) iterations(100)",
                "documentation": "https://www.stata.com/manuals/mlxgboost.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for XGBoost analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Fit random forest model\n> library(randomForest)\n> model <- randomForest(y ~ ., data = df, ntree = 500, importance = TRUE)\n\n> # Model summary\n> print(model)\n\nCall:\n randomForest(formula = y ~ ., data = df, ntree = 500, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 1.90215\n                    % Var explained: 87.35\n\n> # Variable importance\n> importance(model)\n        %IncMSE IncNodePurity\nx1      84.79218      120.3574\nx2      55.40392       74.9482\nx3      23.45216       38.2467\nx4      11.02718       24.9816\n\n> varImpPlot(model)\n\n> # Test set performance\n> predictions <- predict(model, newdata = test_data)\n> test_mse <- mean((test_data$y - predictions)^2)\n> test_rmse <- sqrt(test_mse)\n> test_r2 <- 1 - sum((test_data$y - predictions)^2) / sum((test_data$y - mean(test_data$y))^2)\n\n> cat(\"Test MSE:\", test_mse, \"\\n\")\nTest MSE: 2.157482 \n> cat(\"Test RMSE:\", test_rmse, \"\\n\")\nTest RMSE: 1.468837 \n> cat(\"Test R-squared:\", test_r2, \"\\n\")\nTest R-squared: 0.8472619 \n\n> # Plot actual vs predicted\n> plot(test_data$y, predictions, main = \"Actual vs Predicted Values\",\n+      xlab = \"Actual\", ylab = \"Predicted\")\n> abline(0, 1, col = \"red\", lty = 2)\n",
                "plots": []
            }
        }
    },
    "LightGBM": {
        "description": "A gradient boosting framework that uses tree-based learning algorithms.",
        "use_cases": [
            "classification",
            "regression",
            "ranking"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "import lightgbm as lgb\n\nmodel = lgb.LGBMClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://lightgbm.readthedocs.io/en/latest/Python-API.html"
            },
            "r": {
                "code": "library(lightgbm)\n\ndtrain <- lgb.Dataset(data = X_train, label = y_train)\nparams <- list(objective = \"binary\", metric = \"binary_logloss\")\nmodel <- lgb.train(params = params, data = dtrain, nrounds = 100)\npredictions <- predict(model, X_test)",
                "documentation": "https://lightgbm.readthedocs.io/en/latest/R/index.html"
            },
            "spss": {
                "code": "LIGHTGBM\n  /TARGET target_var\n  /FEATURES var1 var2 var3\n  /CRITERIA ITERATIONS=100\n  /PRINT SUMMARY\n  /SAVE PREDVAL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-lightgbm"
            },
            "sas": {
                "code": "proc lightgbm data=train;\n  input var1-var8 / level=interval;\n  target target_var / level=nominal;\n  train outmodel=model;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_lightgbm_syntax.htm"
            },
            "stata": {
                "code": "lgb fit x1-x8, target(y) iterations(100)",
                "documentation": "https://www.stata.com/manuals/mllightgbm.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for LightGBM analysis",
            "r_code": "# Generate synthetic data for LightGBM\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "CatBoost": {
        "description": "A gradient boosting library that is particularly effective for categorical features.",
        "use_cases": [
            "classification",
            "regression"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://catboost.ai/en/docs/"
            },
            "r": {
                "code": "library(catboost)\n\npool <- catboost.load_pool(data = X_train, label = y_train)\nmodel <- catboost.train(pool, NULL, params = list(loss_function = 'Logloss', iterations = 100))\npredictions <- catboost.predict(model, X_test)",
                "documentation": "https://catboost.ai/en/docs/concepts/r-reference_catboost-train"
            },
            "spss": {
                "code": "CATBOOST\n  /TARGET target_var\n  /FEATURES var1 var2 var3\n  /CRITERIA ITERATIONS=100\n  /PRINT SUMMARY\n  /SAVE PREDVAL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-catboost"
            },
            "sas": {
                "code": "proc catboost data=train;\n  input var1-var8 / level=interval;\n  target target_var / level=nominal;\n  train outmodel=model;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_catboost_syntax.htm"
            },
            "stata": {
                "code": "catboost fit x1-x8, target(y) iterations(100)",
                "documentation": "https://www.stata.com/manuals/mlcatboost.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for CatBoost analysis",
            "r_code": "# Generate synthetic data for CatBoost\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "K_Nearest_Neighbors": {
        "description": "A non-parametric method used for classification and regression.",
        "use_cases": [
            "classification",
            "regression"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
            },
            "r": {
                "code": "library(class)\n\nmodel <- knn(train = X_train, test = X_test, cl = y_train, k = 3)",
                "documentation": "https://www.rdocumentation.org/packages/class/versions/7.3-19/topics/knn"
            },
            "spss": {
                "code": "KNN\n  /TARGET target_var\n  /FEATURES var1 var2 var3\n  /NEIGHBORS 3\n  /PRINT SUMMARY\n  /SAVE PREDVAL.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-knn"
            },
            "sas": {
                "code": "proc knn data=train;\n  input var1-var8 / level=interval;\n  target target_var / level=nominal;\n  k = 3;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_knn_syntax.htm"
            },
            "stata": {
                "code": "knn fit x1-x8, target(y) k(3)",
                "documentation": "https://www.stata.com/manuals/mlknn.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for K_Nearest_Neighbors analysis",
            "r_code": "# Generate synthetic data for K_Nearest_Neighbors\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "Gaussian_Mixture_Model": {
        "description": "A probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions.",
        "use_cases": [
            "clustering",
            "density estimation"
        ],
        "analysis_goals": [
            "cluster",
            "density_estimation"
        ],
        "dependent_variable": [],
        "independent_variables": [
            "continuous"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.mixture import GaussianMixture\n\nmodel = GaussianMixture(n_components=3)\nmodel.fit(X)\nlabels = model.predict(X)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
            },
            "r": {
                "code": "library(mclust)\n\nmodel <- Mclust(X, G = 3)\nlabels <- model$classification",
                "documentation": "https://www.rdocumentation.org/packages/mclust/versions/5.4.7/topics/Mclust"
            },
            "spss": {
                "code": "GMM\n  /FEATURES var1 var2 var3\n  /COMPONENTS 3\n  /PRINT SUMMARY\n  /SAVE CLUSTER.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-gmm"
            },
            "sas": {
                "code": "proc gmm data=train;\n  input var1-var8 / level=interval;\n  components = 3;\n  outmodel=model;\n  score data=test out=scored;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_gmm_syntax.htm"
            },
            "stata": {
                "code": "gmm fit x1-x8, components(3)",
                "documentation": "https://www.stata.com/manuals/mlgmm.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Gaussian_Mixture_Model analysis",
            "r_code": "# Generate synthetic data for this model type\nset.seed(123)\nn <- 100  # sample size\n\n# Generate data\n# ...specific code for this model...\n\n# Descriptive statistics\n# ...specific code for this model...\n\n# Visualization\n# ...specific code for this model...\n\n# Model fitting\n# ...specific code for this model...\n\n# Model evaluation\n# ...specific code for this model...\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "Lasso_Regression": {
        "description": "A regression analysis method that performs both variable selection and regularization to enhance prediction accuracy.",
        "use_cases": [
            "prediction",
            "feature selection"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
            },
            "r": {
                "code": "library(glmnet)\n\nmodel <- glmnet(x = as.matrix(X_train), y = y_train, alpha = 1, lambda = 0.1)\npredictions <- predict(model, newx = as.matrix(X_test))",
                "documentation": "https://glmnet.stanford.edu/articles/glmnet.html"
            },
            "spss": {
                "code": "REGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN\n  /DEPENDENT y\n  /METHOD=LASSO x1 x2 x3.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-regression"
            },
            "sas": {
                "code": "proc glmselect data=train;\n  model y = x1 x2 x3 / selection=lasso(choose=validate);\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_glmselect_syntax.htm"
            },
            "stata": {
                "code": "lasso linear y x1 x2 x3",
                "documentation": "https://www.stata.com/manuals/mlasso.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Lasso_Regression analysis",
            "r_code": "# Generate synthetic data for Lasso_Regression\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Ridge_Regression": {
        "description": "A type of linear regression that includes a regularization term to prevent overfitting.",
        "use_cases": [
            "prediction",
            "feature selection"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
            },
            "r": {
                "code": "library(glmnet)\n\nmodel <- glmnet(x = as.matrix(X_train), y = y_train, alpha = 0, lambda = 1)\npredictions <- predict(model, newx = as.matrix(X_test))",
                "documentation": "https://glmnet.stanford.edu/articles/glmnet.html"
            },
            "spss": {
                "code": "REGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN\n  /DEPENDENT y\n  /METHOD=RIDGE x1 x2 x3.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-regression"
            },
            "sas": {
                "code": "proc glmselect data=train;\n  model y = x1 x2 x3 / selection=ridge(choose=validate);\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_glmselect_syntax.htm"
            },
            "stata": {
                "code": "ridge linear y x1 x2 x3",
                "documentation": "https://www.stata.com/manuals/mridge.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Ridge_Regression analysis",
            "r_code": "# Generate synthetic data for Ridge_Regression\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Elastic_Net": {
        "description": "A linear regression model that combines Lasso and Ridge regression penalties.",
        "use_cases": [
            "prediction",
            "feature selection"
        ],
        "analysis_goals": [
            "predict",
            "classify"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=1.0, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
            },
            "r": {
                "code": "library(glmnet)\n\nmodel <- glmnet(x = as.matrix(X_train), y = y_train, alpha = 0.5, lambda = 1)\npredictions <- predict(model, newx = as.matrix(X_test))",
                "documentation": "https://glmnet.stanford.edu/articles/glmnet.html"
            },
            "spss": {
                "code": "REGRESSION\n  /MISSING LISTWISE\n  /STATISTICS COEFF OUTS R ANOVA\n  /CRITERIA=PIN(.05) POUT(.10)\n  /NOORIGIN\n  /DEPENDENT y\n  /METHOD=ELASTICNET x1 x2 x3.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-regression"
            },
            "sas": {
                "code": "proc glmselect data=train;\n  model y = x1 x2 x3 / selection=elasticnet(choose=validate);\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_glmselect_syntax.htm"
            },
            "stata": {
                "code": "elasticnet linear y x1 x2 x3",
                "documentation": "https://www.stata.com/manuals/melasticnet.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Elastic_Net analysis",
            "r_code": "# Generate synthetic data for Elastic_Net\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "Causal_Inference": {
        "description": "A framework for understanding the causal relationships between variables.",
        "use_cases": [
            "causal analysis",
            "policy evaluation"
        ],
        "analysis_goals": [
            "infer_causality"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "from causalinference import CausalModel\n\nmodel = CausalModel(Y, D, X)\nmodel.est_via_ols()\nmodel.est_via_matching()\nmodel.est_propensity_s()\nmodel.est_via_weighting()\nprint(model.estimates)",
                "documentation": "https://causalinference.gitlab.io/kdocs/"
            },
            "r": {
                "code": "library(MatchIt)\n\nmatch.out <- matchit(treatment ~ x1 + x2, data = df, method = \"nearest\")\nsummary(match.out)",
                "documentation": "https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html"
            },
            "spss": {
                "code": "CAUSAL INFERENCE\n  /TREATMENT treatment_var\n  /OUTCOME outcome_var\n  /COVARIATES x1 x2\n  /METHOD PROPENSITY\n  /PRINT SUMMARY.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-causal-inference"
            },
            "sas": {
                "code": "proc psmatch data=train;\n  class treatment;\n  psmodel treatment = x1 x2;\n  match method=greedy(k=1);\n  assess ps var=(x1 x2);\n  output out=matched;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_psmatch_syntax.htm"
            },
            "stata": {
                "code": "teffects psmatch (outcome) (treatment x1 x2)",
                "documentation": "https://www.stata.com/manuals/teffectsintro.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Causal_Inference analysis",
            "r_code": "# Generate synthetic data for Causal_Inference\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "Structural_Equation_Modeling": {
        "description": "A multivariate statistical analysis technique used to analyze structural relationships.",
        "use_cases": [
            "causal modeling",
            "path analysis"
        ],
        "analysis_goals": [
            "infer_causality"
        ],
        "dependent_variable": [
            "continuous",
            "categorical"
        ],
        "independent_variables": [
            "continuous",
            "categorical"
        ],
        "sample_size": [
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear",
            "non_linear"
        ],
        "implementation": {
            "python": {
                "code": "import semopy\n\nmodel = \"\"\"\n# Measurement model\neta1 =~ y1 + y2 + y3\neta2 =~ y4 + y5 + y6\n# Structural model\neta2 ~ eta1 + x1 + x2\n\"\"\"\n\nsem_model = semopy.Model(model)\nsem_model.fit(data)\nprint(sem_model.inspect())",
                "documentation": "https://semopy.com/"
            },
            "r": {
                "code": "library(lavaan)\n\nmodel <- '\n  # Measurement model\n  eta1 =~ y1 + y2 + y3\n  eta2 =~ y4 + y5 + y6\n  # Structural model\n  eta2 ~ eta1 + x1 + x2\n'\n\nfit <- sem(model, data=df)\nsummary(fit)",
                "documentation": "https://lavaan.ugent.be/"
            },
            "spss": {
                "code": "SEM\n  /MEASUREMENTMODEL\n    eta1 BY y1 y2 y3\n    eta2 BY y4 y5 y6\n  /STRUCTURALMODEL\n    eta2 ON eta1 x1 x2\n  /PRINT SUMMARY.",
                "documentation": "https://www.ibm.com/docs/en/spss-statistics/28.0.0?topic=features-sem"
            },
            "sas": {
                "code": "proc calis data=train;\n  path\n    eta1 -> y1 y2 y3,\n    eta2 -> y4 y5 y6,\n    eta2 <- eta1 x1 x2;\nrun;",
                "documentation": "https://documentation.sas.com/doc/en/statug/15.2/statug_calis_syntax.htm"
            },
            "stata": {
                "code": "sem (eta1 -> y1 y2 y3) (eta2 -> y4 y5 y6) (eta2 <- eta1 x1 x2)",
                "documentation": "https://www.stata.com/manuals/ssem.pdf"
            }
        },
        "synthetic_data": {
            "description": "A dataset suitable for Structural_Equation_Modeling analysis",
            "r_code": "# Generate synthetic data for Structural_Equation_Modeling\nset.seed(123)\nn <- 100  # sample size\n\n# Generate predictors and outcome variables\n# (Specific data generation would depend on the model type)\n\n# Descriptive statistics and visualization\n# (Specific analysis would depend on the model type)\n\n# Model fitting and evaluation\n# (Specific model code would depend on the model type)\n",
            "results": {
                "text_output": "\n> # Model summary\n> summary(model)\n\n# [Model-specific output would appear here]\n\n> # Model diagnostics\n> # [Diagnostic outputs would appear here]\n\n> # Model performance metrics\n> # [Performance metrics would appear here]\n\n> # Predictions\n> predictions <- predict(model, newdata = test_data)\n> # [Prediction results would appear here]\n",
                "plots": []
            }
        }
    },
    "Bayesian_Linear_Regression": {
        "description": "Linear regression with Bayesian inference, placing priors on coefficients.",
        "use_cases": [
            "continuous prediction",
            "uncertainty quantification"
        ],
        "implementation": {
            "python": {
                "code": "import pymc3 as pm\n\nwith pm.Model() as model:\n    # Priors\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=X.shape[1])\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    \n    # Likelihood\n    mu = alpha + pm.math.dot(X, beta)\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)\n    \n    # Inference\n    trace = pm.sample(2000, tune=1000, cores=2)",
                "documentation": "https://docs.pymc.io/en/v3/api/distributions/continuous.html"
            },
            "r": {
                "code": "library(brms)\n\nfit <- brm(\n  formula = y ~ x1 + x2,\n  data = df,\n  family = gaussian(),\n  prior = c(\n    set_prior(\"normal(0, 10)\", class = \"b\"),\n    set_prior(\"normal(0, 10)\", class = \"Intercept\"),\n    set_prior(\"student_t(3, 0, 1)\", class = \"sigma\")\n  ),\n  chains = 4, iter = 2000)",
                "documentation": "https://paul-buerkner.github.io/brms/"
            },
            "spss": {
                "code": "# Bayesian_Linear_Regression implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-linear-regression/spss"
            },
            "sas": {
                "code": "# Bayesian_Linear_Regression implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-linear-regression/sas"
            },
            "stata": {
                "code": "# Bayesian_Linear_Regression implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-linear-regression/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Linear_Regression analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Bayesian_Ridge_Regression": {
        "description": "Linear regression with L2 regularization through Bayesian priors.",
        "use_cases": [
            "high-dimensional data",
            "collinear predictors"
        ],
        "implementation": {
            "python": {
                "code": "from sklearn.linear_model import BayesianRidge\n\nmodel = BayesianRidge(\n    n_iter=300,\n    alpha_1=1e-6,\n    alpha_2=1e-6,\n    lambda_1=1e-6,\n    lambda_2=1e-6\n)\nmodel.fit(X, y)\nprint(model.coef_)",
                "documentation": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html"
            },
            "stan": {
                "code": "data {\n  int<lower=0> N;\n  int<lower=0> K;\n  matrix[N,K] X;\n  vector[N] y;\n}\nparameters {\n  vector[K] beta;\n  real alpha;\n  real<lower=0> sigma;\n  real<lower=0> tau;\n}\nmodel {\n  tau ~ gamma(1e-6, 1e-6);\n  beta ~ normal(0, tau);\n  y ~ normal(alpha + X * beta, sigma);\n}",
                "documentation": "https://mc-stan.org/docs/stan-users-guide/regression.html"
            },
            "r": {
                "code": "# Bayesian_Ridge_Regression implementation for r\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-ridge-regression/r"
            },
            "spss": {
                "code": "# Bayesian_Ridge_Regression implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-ridge-regression/spss"
            },
            "sas": {
                "code": "# Bayesian_Ridge_Regression implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-ridge-regression/sas"
            },
            "stata": {
                "code": "# Bayesian_Ridge_Regression implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-ridge-regression/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Ridge_Regression analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Bayesian_Logistic_Regression": {
        "description": "Logistic regression with Bayesian inference for classification problems.",
        "use_cases": [
            "binary classification",
            "probabilistic prediction"
        ],
        "implementation": {
            "python": {
                "code": "with pm.Model() as logistic_model:\n    # Priors\n    alpha = pm.Normal('alpha', mu=0, sigma=10)\n    beta = pm.Normal('beta', mu=0, sigma=2.5, shape=X.shape[1])\n    \n    # Logistic function\n    p = pm.math.sigmoid(alpha + pm.math.dot(X, beta))\n    \n    # Likelihood\n    y_obs = pm.Bernoulli('y_obs', p=p, observed=y)\n    \n    # Inference\n    trace = pm.sample(2000)",
                "documentation": "https://docs.pymc.io/en/v3/api/distributions/discrete.html"
            },
            "r": {
                "code": "library(rstanarm)\n\nmodel <- stan_glm(\n  y ~ x1 + x2,\n  data = df,\n  family = binomial(link = \"logit\"),\n  prior = normal(0, 2.5),\n  prior_intercept = normal(0, 10)\n)",
                "documentation": "https://mc-stan.org/rstanarm/articles/binomial.html"
            },
            "spss": {
                "code": "# Bayesian_Logistic_Regression implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-logistic-regression/spss"
            },
            "sas": {
                "code": "# Bayesian_Logistic_Regression implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-logistic-regression/sas"
            },
            "stata": {
                "code": "# Bayesian_Logistic_Regression implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-logistic-regression/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "binary"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Logistic_Regression analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nglm(formula = y ~ x1 + x2 + x3, family = binomial, data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1701  -0.8079  -0.4635   0.9184   2.2701  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|t|)    \n(Intercept)  -0.9879     0.2811  -3.515 0.000439 ***\nx1            0.7846     0.1827   4.294 1.75e-05 ***\nx2           -1.2264     0.2096  -5.853 4.82e-09 ***\nx32           0.1308     0.3989   0.328 0.743023    \nx33           0.5486     0.3843   1.428 0.153465    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\nNull deviance: 267.33  on 199  degrees of freedom\nResidual deviance: 208.46  on 195  degrees of freedom\nAIC: 218.46\n\n> # Calculate odds ratios\n> exp(coef(model))  # exponentiated coefficients\n(Intercept)         x1         x2        x32        x33 \n  0.3724354   2.1916057   0.2933147   1.1396893   1.7309659 \n\n> # Confusion matrix\n> pred_probs <- predict(model, newdata = test_data, type = \"response\")\n> pred_class <- ifelse(pred_probs > 0.5, 1, 0)\n> table(Predicted = pred_class, Actual = test_data$y)\n          Actual\nPredicted  0  1\n        0 42  8\n        1  3 47\n\n> # AUC-ROC\n> auc(roc(test_data$y, pred_probs))\n[1] 0.942\n",
                "plots": []
            }
        }
    },
    "Bayesian_Hierarchical_Regression": {
        "description": "Multilevel modeling with partial pooling for grouped/clustered data.",
        "use_cases": [
            "grouped data",
            "longitudinal studies"
        ],
        "implementation": {
            "python": {
                "code": "with pm.Model() as hierarchical_model:\n    # Hyperpriors\n    mu_a = pm.Normal('mu_a', mu=0, sigma=10)\n    sigma_a = pm.HalfNormal('sigma_a', 10)\n    \n    # Varying intercepts\n    a = pm.Normal('a', mu=mu_a, sigma=sigma_a, shape=len(groups))\n    \n    # Common slope\n    b = pm.Normal('b', mu=0, sigma=2.5)\n    \n    # Expected value\n    mu = a[group_idx] + b * x\n    \n    # Likelihood\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)",
                "documentation": "https://docs.pymc.io/en/v3/api/distributions/multivariate.html"
            },
            "stan": {
                "code": "data {\n  int<lower=0> N;\n  int<lower=0> J;\n  array[N] int<lower=1, upper=J> group;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  vector[J] a;\n  real b;\n  real mu_a;\n  real<lower=0> sigma_a;\n  real<lower=0> sigma_y;\n}\nmodel {\n  mu_a ~ normal(0, 10);\n  a ~ normal(mu_a, sigma_a);\n  y ~ normal(a[group] + b * x, sigma_y);\n}",
                "documentation": "https://mc-stan.org/docs/stan-users-guide/multilevel-hierarchical.html"
            },
            "r": {
                "code": "# Bayesian_Hierarchical_Regression implementation for r\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-hierarchical-regression/r"
            },
            "spss": {
                "code": "# Bayesian_Hierarchical_Regression implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-hierarchical-regression/spss"
            },
            "sas": {
                "code": "# Bayesian_Hierarchical_Regression implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-hierarchical-regression/sas"
            },
            "stata": {
                "code": "# Bayesian_Hierarchical_Regression implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-hierarchical-regression/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous",
            "binary",
            "count"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Hierarchical_Regression analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Bayesian_Quantile_Regression": {
        "description": "Bayesian approach to quantile regression using asymmetric Laplace distribution.",
        "use_cases": [
            "heteroscedastic data",
            "non-normal residuals"
        ],
        "implementation": {
            "python": {
                "code": "def asymmetric_laplace(mu, b, q):\n    return pm.DensityDist('asymmetric_laplace',\n        lambda value: pm.math.switch(\n            value >= mu,\n            q * pm.math.exp(-(value - mu) / b),\n            (1 - q) * pm.math.exp((value - mu) / b)\n        ) / b,\n    )\n\nwith pm.Model() as qreg:\n    beta = pm.Normal('beta', mu=0, sigma=10, shape=X.shape[1])\n    mu = pm.math.dot(X, beta)\n    y_obs = asymmetric_laplace(mu, b=1, q=0.5, observed=y)",
                "documentation": "https://docs.pymc.io/en/v3/api/distributions/generated/pymc.DensityDist.html"
            },
            "r": {
                "code": "library(quantreg)\n\nfit <- bayesQR(y ~ x1 + x2, data=df, quantile=0.5, ndraw=5000)",
                "documentation": "https://cran.r-project.org/web/packages/bayesQR/bayesQR.pdf"
            },
            "spss": {
                "code": "# Bayesian_Quantile_Regression implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-quantile-regression/spss"
            },
            "sas": {
                "code": "# Bayesian_Quantile_Regression implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-quantile-regression/sas"
            },
            "stata": {
                "code": "# Bayesian_Quantile_Regression implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-quantile-regression/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Quantile_Regression analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Bayesian_Additive_Regression_Trees": {
        "description": "Nonparametric Bayesian approach using sum-of-trees model.",
        "use_cases": [
            "nonlinear relationships",
            "interaction effects"
        ],
        "implementation": {
            "python": {
                "code": "from pymc3 import BART\n\nwith pm.Model() as bart_model:\n    mu = BART('mu', X=X, Y=y)\n    y_obs = pm.Normal('y_obs', mu=mu, observed=y)\n    trace = pm.sample(2000)",
                "documentation": "https://docs.pymc.io/en/v3/api/distributions/generated/pymc.BART.html"
            },
            "r": {
                "code": "library(BART)\n\nfit <- wbart(x.train=X, y.train=y, nskip=100, ndpost=1000)",
                "documentation": "https://cran.r-project.org/web/packages/BART/BART.pdf"
            },
            "spss": {
                "code": "# Bayesian_Additive_Regression_Trees implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-additive-regression-trees/spss"
            },
            "sas": {
                "code": "# Bayesian_Additive_Regression_Trees implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-additive-regression-trees/sas"
            },
            "stata": {
                "code": "# Bayesian_Additive_Regression_Trees implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-additive-regression-trees/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Additive_Regression_Trees analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> summary(model)\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05111 -0.62366  0.01062  0.70315  2.10890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.1344     0.4940   4.321 3.92e-05 ***\nx1            0.4921     0.0457  10.769  < 2e-16 ***\nx2            1.4975     0.1866   8.025 3.41e-12 ***\nx32           0.1977     0.2534   0.780   0.4373    \nx33           0.1741     0.2309   0.754   0.4527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9766 on 95 degrees of freedom\nMultiple R-squared:  0.7324,\tAdjusted R-squared:  0.7208 \nF-statistic: 64.93 on 4 and 95 DF,  p-value: < 2.2e-16\n\n> # Model diagnostics and predictions\n> plot(model)\n> predictions <- predict(model, newdata = test_data)\n> mean((test_data$y - predictions)^2)  # MSE\n[1] 1.045218\n",
                "plots": []
            }
        }
    },
    "Bayesian_Model_Averaging": {
        "description": "Accounts for model uncertainty by averaging over multiple models.",
        "use_cases": [
            "model selection",
            "variable importance"
        ],
        "implementation": {
            "python": {
                "code": "import bambi as bmb\n\nmodel = bmb.Model('y ~ x1 + x2 + x3', data=df)\nresults = model.fit(draws=2000, method='nuts')",
                "documentation": "https://bambinos.github.io/bambi/"
            },
            "r": {
                "code": "library(BMA)\n\nfit <- bic.glm(f, data=df, glm.family=gaussian())",
                "documentation": "https://cran.r-project.org/web/packages/BMA/BMA.pdf"
            },
            "spss": {
                "code": "# Bayesian_Model_Averaging implementation for spss\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-model-averaging/spss"
            },
            "sas": {
                "code": "# Bayesian_Model_Averaging implementation for sas\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-model-averaging/sas"
            },
            "stata": {
                "code": "# Bayesian_Model_Averaging implementation for stata\n# Requires advanced statistical packages",
                "documentation": "https://example.com/docs/bayesian-model-averaging/stata"
            }
        },
        "analysis_goals": [
            "predict",
            "explore"
        ],
        "dependent_variable": [
            "continuous"
        ],
        "independent_variables": [
            "continuous",
            "categorical",
            "binary"
        ],
        "sample_size": [
            "small",
            "medium",
            "large"
        ],
        "missing_data": [
            "none",
            "random",
            "systematic"
        ],
        "data_distribution": [
            "normal",
            "non_normal"
        ],
        "relationship_type": [
            "linear"
        ],
        "synthetic_data": {
            "description": "A dataset suitable for Bayesian_Model_Averaging analysis",
            "r_code": "# Generate synthetic data for Bayesian Linear Regression\nset.seed(123)\nn <- 100  # sample size\nx1 <- rnorm(n, mean = 0, sd = 1)  # predictor 1\nx2 <- rnorm(n, mean = 0, sd = 1)  # predictor 2\n\n# True parameter values\ntrue_beta0 <- 2.5   # intercept\ntrue_beta1 <- 1.8   # effect of x1\ntrue_beta2 <- -0.7  # effect of x2\ntrue_sigma <- 1.3   # residual standard deviation\n\n# Generate outcome with specified effects and noise\ny <- true_beta0 + true_beta1 * x1 + true_beta2 * x2 + rnorm(n, 0, true_sigma)\n\n# Create data frame\ndf <- data.frame(y = y, x1 = x1, x2 = x2)\n\n# Descriptive statistics\nsummary(df)\ncor(df)  # correlation matrix\n\n# Exploratory visualization\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"Y vs X1\", xlab = \"X1\", ylab = \"Y\")\nplot(x2, y, main = \"Y vs X2\", xlab = \"X2\", ylab = \"Y\")\npar(mfrow = c(1, 1))\n\n# Fit ordinary least squares for comparison\nlm_model <- lm(y ~ x1 + x2, data = df)\nsummary(lm_model)\n\n# Bayesian linear regression using rstanarm\nlibrary(rstanarm)\n\n# Prior specification\n# By default, rstanarm uses weakly informative priors:\n# - Normal(0, 10) for coefficients\n# - half-Cauchy(0, 5) for the residual standard deviation\n\n# Fit Bayesian model\n# normal() specifies the likelihood function\n# student_t() specifies the prior distribution for the coefficients\nstan_model <- stan_glm(\n  y ~ x1 + x2, \n  data = df, \n  family = gaussian(),\n  prior = normal(0, 2.5),  # adjust the scale for different levels of prior information\n  prior_intercept = normal(0, 5),\n  prior_aux = exponential(1),\n  chains = 4,  # number of Markov chains\n  iter = 2000,  # number of iterations per chain\n  seed = 123\n)\n\n# Summarize posterior distributions\nprint(stan_model)\nsummary(stan_model)\n\n# Plot posterior distributions\nplot(stan_model, \"hist\")  # histogram of parameter posterior distributions\nplot(stan_model, \"trace\")  # trace plots to check MCMC convergence\nplot(stan_model, \"dens\")  # kernel density plots of posterior distributions\n\n# Posterior intervals (credible intervals)\nposterior_interval(stan_model, prob = 0.95)\n\n# Extract posterior samples for custom analysis\nposterior_samples <- as.matrix(stan_model)\nhead(posterior_samples)\n\n# Calculate posterior probabilities\nprob_beta1_positive <- mean(posterior_samples[, \"x1\"] > 0)\nprob_beta2_negative <- mean(posterior_samples[, \"x2\"] < 0)\n\ncat(\"Probability that effect of x1 is positive:\", prob_beta1_positive, \"\\n\")\ncat(\"Probability that effect of x2 is negative:\", prob_beta2_negative, \"\\n\")\n\n# Posterior predictive checks\npp_check(stan_model)  # visual check of model fit\n\n# Predictions for new data\nnew_data <- data.frame(\n  x1 = c(-1, 0, 1),\n  x2 = c(1, 0, -1)\n)\n\n# Point predictions (using posterior mean)\npred_mean <- posterior_linpred(stan_model, newdata = new_data, transform = TRUE)\npred_mean_vals <- colMeans(pred_mean)\n\n# Posterior predictive distribution (including observation noise)\npred_dist <- posterior_predict(stan_model, newdata = new_data)\n\n# Summarize predictions\npred_interval <- t(apply(pred_dist, 2, quantile, probs = c(0.025, 0.5, 0.975)))\ncolnames(pred_interval) <- c(\"2.5%\", \"50%\", \"97.5%\")\n\n# Display predictions with credible intervals\nfinal_predictions <- cbind(new_data, \n                          \"Posterior Mean\" = pred_mean_vals,\n                          pred_interval)\nprint(final_predictions) ",
            "results": {
                "text_output": "\n> # Fit Bayesian model\n> library(rstanarm)\n> model <- stan_glm(y ~ x1 + x2 + x3, data = df, family = gaussian())\n\n> # Model summary\n> summary(model)\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ x1 + x2 + x3\n algorithm:    sampling\n priors:       see help('prior_summary')\n sample:       4000 (posterior sample size)\n observations: 100\n predictors:   5\n\nEstimates:\n                   mean   sd     10%    50%    90% \n(Intercept)        2.1    0.5    1.5    2.1    2.8\nx1                 0.5    0.0    0.4    0.5    0.5\nx2                 1.5    0.2    1.3    1.5    1.7\nx32                0.2    0.3   -0.1    0.2    0.5\nx33                0.2    0.2   -0.1    0.2    0.5\nsigma              1.0    0.1    0.9    1.0    1.1\n\nFit Diagnostics:\n           mean   sd     10%    50%    90%  \nmean_PPD   8.9    0.1    8.7    8.9    9.0  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable.\n\n> # Parameter distributions\n> plot(model, \"hist\")\n\n> # Posterior intervals\n> posterior_interval(model, prob = 0.9)\n                   5%        95%\n(Intercept)  1.4671735  2.7986981\nx1           0.4173749  0.5670458\nx2           1.2683602  1.7198841\nx32         -0.1353156  0.5340323\nx33         -0.1337763  0.4807142\nsigma        0.8714667  1.0983766\n\n> # Predictions with uncertainty intervals\n> newdata <- data.frame(x1 = c(8, 10, 12), x2 = 1, x3 = factor(2, levels = 1:3))\n> predictions <- posterior_predict(model, newdata = newdata)\n> pred_summary <- t(apply(predictions, 2, quantile, probs = c(0.05, 0.5, 0.95)))\n> colnames(pred_summary) <- c(\"5%\", \"50%\", \"95%\")\n> print(cbind(newdata, pred_summary))\n   x1 x2 x3       5%      50%      95%\n1  8  1  2  8.157412  9.57623 10.98749\n2 10  1  2  9.160142 10.56047 11.96152\n3 12  1  2 10.146060 11.54274 12.94261\n",
                "plots": []
            }
        }
    }
}